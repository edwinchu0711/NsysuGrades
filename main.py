import os
import cv2
import base64
import numpy as np
import tensorflow as tf
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import requests
from bs4 import BeautifulSoup
import re
import json
from flask import Flask, request, jsonify

app = Flask(__name__)

# =====================
# åŸºæœ¬è¨­å®š (éœ€èˆ‡è¨“ç·´ç«¯å®Œå…¨ä¸€è‡´)
# =====================
IMG_WIDTH = 124
IMG_HEIGHT = 24
DIGITS = 4
CHARACTERS = "0123456789"
TFLITE_NAME = "model.tflite"

# =====================
# è¼‰å…¥ TFLite æ¨¡å‹
# =====================
def load_tflite_model():
    current_dir = os.path.dirname(__file__)
    model_path = os.path.join(current_dir, TFLITE_NAME)
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° TFLite æ¨¡å‹æª”ï¼š{model_path}")

    with open(model_path, "rb") as f:
        model_bytes = f.read()

    interpreter = tf.lite.Interpreter(model_content=model_bytes)
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print("âœ… TFLite æ¨¡å‹è¼‰å…¥å®Œæˆ")
    return interpreter, input_details, output_details

# =====================
# åœ–ç‰‡é è™•ç† (å®Œå…¨åŒæ­¥è¨“ç·´ç«¯é‚è¼¯)
# =====================
def preprocess_image_for_model(image_path):
    # 1. è®€å–åœ–ç‰‡
    img = cv2.imread(image_path)
    if img is None:
        raise ValueError(f"ç„¡æ³•è®€å–åœ–ç‰‡ï¼š{image_path}")

    # 2. è½‰ç°éš
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # 3. è‡ªé©æ‡‰äºŒå€¼åŒ–ï¼šè®“å­—æ›´å‡¸é¡¯ï¼Œå­—ç‚ºç™½ï¼ŒèƒŒæ™¯ç‚ºé»‘
    bin_img = cv2.adaptiveThreshold(
        gray,
        255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        11,
        2
    )

    # 4. å½¢æ…‹å­¸æ“ä½œï¼šå»å™ªé»
    kernel = np.ones((2, 2), np.uint8)
    bin_img = cv2.morphologyEx(bin_img, cv2.MORPH_OPEN, kernel)

    # 5. Resize åˆ°çµ±ä¸€å¤§å°
    bin_img = cv2.resize(bin_img, (IMG_WIDTH, IMG_HEIGHT))

    # 6. æ­£è¦åŒ– + ç¶­åº¦æ“´å……
    bin_img = bin_img.astype(np.float32) / 255.0
    bin_img = np.expand_dims(bin_img, axis=-1)  # (H, W, 1)
    bin_img = np.expand_dims(bin_img, axis=0)   # (1, H, W, 1)
    
    return bin_img

# =====================
# é æ¸¬å‡½å¼
# =====================
def predict_captcha(interpreter, input_details, output_details, image_path):
    input_data = preprocess_image_for_model(image_path)

    # è¨­å®šè¼¸å…¥æ•¸å€¼
    interpreter.set_tensor(input_details[0]["index"], input_data)
    interpreter.invoke()

    # é—œéµï¼šæ ¹æ“šè¼¸å‡ºçš„åç¨±æ’åº (ç¢ºä¿ digit0 åœ¨ç¬¬ä¸€ä½)
    sorted_outputs = sorted(output_details, key=lambda x: x['name'])

    digits = []
    for od in sorted_outputs:
        probs = interpreter.get_tensor(od["index"])
        idx = int(np.argmax(probs, axis=-1)[0])
        digits.append(CHARACTERS[idx])

    # --- æ–°å¢ï¼šè¾¨è­˜å®Œæˆå¾Œåˆªé™¤æš«å­˜åœ–ç‰‡ ---
    if os.path.exists(image_path):
        os.remove(image_path)
    return "".join(digits)


def login(driver, interpreter, input_details, output_details, account, password):
    try:
        # 2. ç²å–é©—è­‰ç¢¼åœ–ç‰‡
        captcha_element = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "imgVC"))
        )
        captcha_base64 = captcha_element.screenshot_as_base64
        img_path = "temp_captcha.png"
        with open(img_path, "wb") as f:
            f.write(base64.b64decode(captcha_base64))
        
        code = predict_captcha(interpreter, input_details, output_details, img_path)
        print(f"è¾¨è­˜çµæœ: {code}")

        # 3. è¼¸å…¥å¸³è™Ÿ
        acc_input = driver.find_element(By.CSS_SELECTOR, 'input[name="SID"]')
        acc_input.clear()
        acc_input.send_keys(account)

        # 4. è¼¸å…¥å¯†ç¢¼
        pwd_input = driver.find_element(By.CSS_SELECTOR, 'input[name="PASSWD"]')
        pwd_input.clear()
        pwd_input.send_keys(password)

        # 5. å¡«å…¥é©—è­‰ç¢¼
        valid_input = driver.find_element(By.NAME, "ValidCode")
        valid_input.clear()
        valid_input.send_keys(code)

        # 6. æŒ‰ä¸‹ã€Œç¢ºå®šé€å‡ºã€æŒ‰éˆ•
        submit_btn = driver.find_element(By.CSS_SELECTOR, 'input.login_btn_01[wfd-id="id6"]')
        submit_btn.click()
        
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")


def get_requests_session_with_cookies(driver):
    """
    å°‡ Selenium çš„ Cookie æ³¨å…¥åˆ°ä¸€å€‹çœŸæ­£çš„ requests.Session ç‰©ä»¶ä¸­
    """
    session = requests.Session()
    selenium_cookies = driver.get_cookies()
    for cookie in selenium_cookies:
        session.cookies.set(cookie['name'], cookie['value'])
    return session

def scrape_all_courses(driver):
    print("\nğŸš€ é–‹å§‹åŸ·è¡Œã€é–‹æ”¾æˆç¸¾ã€é«˜é€Ÿçˆ¬å–æ¨¡å¼ (Requests)...")
    
    # 1. æº–å‚™åŸºç¤è³‡æ–™
    list_url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=813&KIND=1&LANGS=cht"
    query_url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?ACTION=814&KIND=1&LANGS=cht"
    
    session = get_requests_session_with_cookies(driver)
    
    headers = {
        "User-Agent": driver.execute_script("return navigator.userAgent;"),
        "Referer": list_url,
        "Content-Type": "application/x-www-form-urlencoded",
        "Origin": "https://selcrs.nsysu.edu.tw"
    }

    # 2. å–å¾—èª²ç¨‹æ¸…å–®
    course_info_map = {}
    try:
        res_list = session.get(list_url, headers=headers, timeout=10)
        res_list.encoding = 'utf-8' 
        soup_list = BeautifulSoup(res_list.text, 'html.parser')
        
        rows = soup_list.find_all('tr')[1:] 
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                course_no = cols[1].get_text(strip=True)
                course_name = cols[2].get_text(strip=True)
                
                if course_no and not re.search(r'[\u4e00-\u9fff]', course_no):
                    course_info_map[course_no] = course_name

        print(f"âœ… æˆåŠŸå–å¾—èª²ç¨‹æ¸…å–®ï¼Œå…± {len(course_info_map)} é–€èª²ç¨‹ã€‚")

    except Exception as e:
        print(f"âŒ å–å¾—æ¸…å–®å¤±æ•—: {e}")
        return []

    # 3. é€ä¸€æŠ“å–è©³ç´°æˆç¸¾
    score_data = [] 

    for course_no, course_name in course_info_map.items():
        print(f"æ­£åœ¨æŠ“å–: [{course_no}] {course_name}      ", end="\r")
        
        payload = {"CRSNO": course_no, "SCO_TYP_COD": "--"}
        
        try:
            response = session.post(query_url, headers=headers, data=payload, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            rows = soup.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 6:
                    text_cols = [c.get_text(strip=True) for c in cols]
                    if text_cols[0].isdigit():
                        score_data.append({
                            "èª²ç¨‹åç¨±": course_name,
                            "å­¸å¹´åº¦": text_cols[0],
                            "å­¸æœŸ": text_cols[1],
                            "æˆç¸¾é …ç›®": text_cols[2],
                            "ç™¾åˆ†æ¯”": text_cols[3],
                            "åŸå§‹åˆ†æ•¸": text_cols[4],
                            "ç­‰ç¬¬æˆç¸¾": text_cols[5],
                            "å‚™è¨»": text_cols[6] if len(text_cols) > 6 else ""
                        })

        except Exception as e:
            print(f"\nâŒ æŠ“å– {course_no} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    print("\nâœ… é–‹æ”¾æˆç¸¾çˆ¬å–ä»»å‹™å®Œæˆã€‚")
    return score_data


def get_selenium_cookies(driver):
    """å°‡ Selenium çš„ Cookie è½‰æ›ç‚º requests å¯ç”¨çš„æ ¼å¼"""
    selenium_cookies = driver.get_cookies()
    cookies = {cookie['name']: cookie['value'] for cookie in selenium_cookies}
    return cookies

def scrape_historical_data(driver):
    """
    é‡å¯«ï¼šä½¿ç”¨ requests é€²è¡Œæ­·å²æˆç¸¾çˆ¬å–
    """
    # 1. å¾ Selenium ç²å–ç™»å…¥å¾Œçš„ Session
    session_cookies = get_selenium_cookies(driver)
    
    # 2. æº–å‚™ Requests ç’°å¢ƒ
    url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?ACTION=804&KIND=2&LANGS=cht"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36",
        "Referer": "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=702",
        "Content-Type": "application/x-www-form-urlencoded"
    }

    grades_data = []
    rank_data = []
    
    years = ["113", "112", "111", "110"] 
    sems = ["0", "1", "2", "3"]

    print("ğŸš€ é–‹å§‹ä½¿ç”¨ Requests æ‰¹æ¬¡æŠ“å–æ­·å²æˆç¸¾...")

    for year in years:
        for sem in sems:
            print(f"æ­£åœ¨æŠ“å–ï¼š{year}å­¸å¹´åº¦ ç¬¬{sem}å­¸æœŸ...")
            payload = f"SYEAR={year}&SEM={sem}"
            
            try:
                response = requests.post(url, headers=headers, cookies=session_cookies, data=payload, timeout=10)
                response.encoding = response.apparent_encoding 
                
                if "ç„¡æ­¤å­¸æœŸæˆç¸¾" in response.text:
                    print(f"â„¹ï¸ {year}-{sem} ç„¡æˆç¸¾è³‡æ–™")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # --- A. è™•ç†æˆç¸¾è¡¨ ---
                rows = soup.find_all('tr')
                for row in rows:
                    cols = [td.get_text(strip=True) for td in row.find_all('td')]
                    
                    if len(cols) >= 6 and year in cols[0]:
                        grades_data.append({
                            "å­¸å¹´åº¦": year,
                            "å­¸æœŸ": sem,
                            "å­¸å¹´åº¦_åŸ": cols[0],
                            "å­¸æœŸ_åŸ": cols[1],
                            "èª²ç¨‹ç·¨è™Ÿ": cols[2],
                            "èª²ç¨‹åç¨±": cols[3],
                            "å­¸åˆ†æ•¸": cols[4],
                            "æˆç¸¾": cols[5]
                        })

                # --- B. è™•ç†æ’åçµ±è¨ˆè¡¨ ---
                if "ä¿®ç¿’å­¸åˆ†" in response.text:
                    for table in soup.find_all('table'):
                        if "ä¿®ç¿’å­¸åˆ†" in table.get_text():
                            rank_info = extract_rank_info(year, sem, table.get_text())
                            if rank_info:
                                rank_data.append(rank_info)
                            break

            except Exception as e:
                print(f"âŒ æŠ“å– {year}-{sem} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return grades_data, rank_data


def extract_rank_info(year, sem, raw_table_text):
    """æå–æ’åè³‡è¨Š"""
    patterns = {
        "ä¿®ç¿’å­¸åˆ†": r"ä¿®ç¿’å­¸åˆ†ï¼š(\d+)",
        "å¯¦å¾—å­¸åˆ†": r"å¯¦å¾—å­¸åˆ†ï¼š(\d+)",
        "å¹³å‡åˆ†æ•¸": r"å¹³å‡åˆ†æ•¸ï¼š([\d\.]+)",
        "æœ¬å­¸æœŸåæ¬¡": r"æœ¬å­¸æœŸåæ¬¡ï¼š(\d+)",
        "å…¨ç­äººæ•¸": r"å…¨ç­äººæ•¸ï¼š(\d+)"
    }

    clean_text = raw_table_text.replace("\n", " ").replace("&nbsp;", "")
    
    rank_info = {"å­¸å¹´åº¦": year, "å­¸æœŸ": sem}
    
    for key, p in patterns.items():
        match = re.search(p, clean_text)
        rank_info[key] = match.group(1) if match else "ç„¡"
    
    return rank_info


def setup_chrome_driver():
    """è¨­å®š Chrome Driver for Render"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    
    # Render ç’°å¢ƒæœƒè‡ªå‹•æä¾› chromedriver
    driver = webdriver.Chrome(options=options)
    return driver


def perform_scraping(account, password):
    """åŸ·è¡Œçˆ¬èŸ²ä¸»æµç¨‹"""
    try:
        interpreter, input_details, output_details = load_tflite_model()
        driver = setup_chrome_driver()

        url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query_login.asp"
        driver.get(url)
        
        # é–‹å§‹ç™»å…¥
        login_successful = False
        max_attempts = 5
        attempts = 0
        
        while not login_successful and attempts < max_attempts:
            attempts += 1
            login(driver, interpreter, input_details, output_details, account, password)
            try:
                WebDriverWait(driver, 1.5).until(EC.alert_is_present())
                alert = driver.switch_to.alert
                alert_text = alert.text
                print(f"è­¦ç¤ºè¨Šæ¯: {alert_text}")
                if "é©—è­‰ç¢¼éŒ¯èª¤" in alert_text or "Verified Code Error" in alert_text:
                    print("âŒ è¾¨è­˜éŒ¯èª¤ï¼Œæ­£åœ¨é—œé–‰è¦–çª—ä¸¦é‡æ–°å˜—è©¦...")
                    alert.accept()
                    driver.get(url)
                    continue
                else:
                    print(f"ç™»å…¥å¤±æ•—ï¼ŒåŸå› : {alert_text}")
                    alert.accept()
                    driver.quit()
                    return {"error": alert_text}
            except:
                print("âœ… æœªåµæ¸¬åˆ°éŒ¯èª¤å½ˆçª—ï¼Œæª¢æŸ¥æ˜¯å¦æˆåŠŸé€²å…¥ç³»çµ±...")
                login_successful = True

        if not login_successful:
            driver.quit()
            return {"error": "ç™»å…¥å¤±æ•—ï¼Œè¶…éæœ€å¤§å˜—è©¦æ¬¡æ•¸"}

        # é–‹æ”¾æˆç¸¾æŸ¥è©¢
        score_link = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=700&KIND=1&LANGS=cht"
        driver.get(score_link)
        score_data = scrape_all_courses(driver)

        # å­¸æœŸæˆç¸¾æŸ¥è©¢
        grades_link = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=700&KIND=2&LANGS=cht"
        driver.get(grades_link)
        grades_data, rank_data = scrape_historical_data(driver)

        driver.quit()

        return {
            "success": True,
            "é–‹æ”¾æˆç¸¾": score_data,
            "å­¸æœŸæˆç¸¾": grades_data,
            "æ’åè³‡è¨Š": rank_data
        }

    except Exception as e:
        return {"error": str(e)}


# =====================
# Flask API è·¯ç”±
# =====================
@app.route('/api/scrape', methods=['POST'])
def scrape():
    """
    API ç«¯é»ï¼šæ¥æ”¶å¸³è™Ÿå¯†ç¢¼ï¼Œè¿”å›çˆ¬å–çµæœ
    
    Request Body:
    {
        "account": "å­¸è™Ÿ",
        "password": "å¯†ç¢¼"
    }
    """
    data = request.get_json()
    
    if not data or 'account' not in data or 'password' not in data:
        return jsonify({"error": "è«‹æä¾› account å’Œ password"}), 400
    
    account = data['account']
    password = data['password']
    
    result = perform_scraping(account, password)
    
    if "error" in result:
        return jsonify(result), 500
    
    return jsonify(result), 200


@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return jsonify({"status": "ok"}), 200


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)
