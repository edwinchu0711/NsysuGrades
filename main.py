import os
import cv2
import base64
import numpy as np
import tensorflow as tf
import requests
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# =====================
# å…¨åŸŸè¨­å®š
# =====================
IMG_WIDTH = 124
IMG_HEIGHT = 24
DIGITS = 4
CHARACTERS = "0123456789"
TFLITE_NAME = "model.tflite"

app = FastAPI()

# =====================
# Pydantic æ¨¡å‹
# =====================
class CrawlRequest(BaseModel):
    account: str
    password: str
    task: str  # "score", "grades", "both", "test"

# =====================
# è¼”åŠ©å‡½å¼ï¼šæ¨¡å‹è¼‰å…¥èˆ‡é æ¸¬
# =====================
def load_tflite_model():
    current_dir = os.path.dirname(__file__)
    model_path = os.path.join(current_dir, TFLITE_NAME)
    
    if not os.path.exists(model_path):
        # ç‚ºäº†é¿å… API å•Ÿå‹•å¤±æ•—ï¼Œé€™è£¡åƒ… print è­¦å‘Šï¼Œå¯¦éš›å‘¼å«æ™‚è‹¥ç„¡æ¨¡å‹æœƒå ±éŒ¯
        print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° TFLite æ¨¡å‹æª”ï¼š{model_path}")
        return None, None, None

    with open(model_path, "rb") as f:
        model_bytes = f.read()

    interpreter = tf.lite.Interpreter(model_content=model_bytes)
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    return interpreter, input_details, output_details

def preprocess_image_from_bytes(img_bytes):
    """
    ä¿®æ”¹ç‰ˆï¼šç›´æ¥å¾è¨˜æ†¶é«” Bytes è™•ç†åœ–ç‰‡ï¼Œä¸è®€å¯«ç¡¬ç¢Ÿ
    """
    # å°‡ bytes è½‰æ›ç‚º numpy array
    nparr = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    if img is None:
        raise ValueError("ç„¡æ³•è§£æåœ–ç‰‡æ•¸æ“š")

    # è½‰ç°éš
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # è‡ªé©æ‡‰äºŒå€¼åŒ–
    bin_img = cv2.adaptiveThreshold(
        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV, 11, 2
    )

    # å½¢æ…‹å­¸æ“ä½œ
    kernel = np.ones((2, 2), np.uint8)
    bin_img = cv2.morphologyEx(bin_img, cv2.MORPH_OPEN, kernel)

    # Resize
    bin_img = cv2.resize(bin_img, (IMG_WIDTH, IMG_HEIGHT))

    # æ­£è¦åŒ– + ç¶­åº¦æ“´å……
    bin_img = bin_img.astype(np.float32) / 255.0
    bin_img = np.expand_dims(bin_img, axis=-1)  # (H, W, 1)
    bin_img = np.expand_dims(bin_img, axis=0)   # (1, H, W, 1)
    
    return bin_img

def predict_captcha(interpreter, input_details, output_details, img_base64_str):
    """
    æ¥æ”¶ Base64 å­—ä¸²ï¼Œé€²è¡Œé æ¸¬
    """
    # è§£ç¢¼ Base64
    img_bytes = base64.b64decode(img_base64_str)
    input_data = preprocess_image_from_bytes(img_bytes)

    # è¨­å®šè¼¸å…¥æ•¸å€¼
    interpreter.set_tensor(input_details[0]["index"], input_data)
    interpreter.invoke()

    # æ’åºè¼¸å‡º
    sorted_outputs = sorted(output_details, key=lambda x: x['name'])

    digits = []
    for od in sorted_outputs:
        probs = interpreter.get_tensor(od["index"])
        idx = int(np.argmax(probs, axis=-1)[0])
        digits.append(CHARACTERS[idx])

    return "".join(digits)

# =====================
# Selenium é©…å‹•èˆ‡ç™»å…¥
# =====================
def get_driver():
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    # API æ¨¡å¼å¼·çƒˆå»ºè­°é–‹å•Ÿ headless
    options.add_argument("--headless") 
    
    # è‹¥åœ¨ Docker æˆ– Linux ç’°å¢ƒï¼Œè·¯å¾‘éœ€è‡ªè¡Œèª¿æ•´ï¼›Windows å¯è¨»è§£æ‰æˆ–æŒ‡å®šè·¯å¾‘
    # chrome_driver_path = r"path/to/chromedriver"
    # service = Service(chrome_driver_path)
    
    # é€™è£¡å‡è¨­å·²å®‰è£ chromedriver æ–¼ç³»çµ±è·¯å¾‘ï¼Œç›´æ¥åˆå§‹åŒ–
    driver = webdriver.Chrome(options=options)
    return driver

def login_process(driver, interpreter, input_details, output_details, account, password):
    """
    ç™»å…¥æµç¨‹ï¼Œå›å‚³ (æ˜¯å¦æˆåŠŸ, è¨Šæ¯)
    """
    url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query_login.asp"
    driver.get(url)
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            # 1. ç²å–é©—è­‰ç¢¼åœ–ç‰‡
            captcha_element = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.NAME, "imgVC"))
            )
            captcha_base64 = captcha_element.screenshot_as_base64
            
            # 2. è¾¨è­˜
            code = predict_captcha(interpreter, input_details, output_details, captcha_base64)
            print(f"å˜—è©¦ç™»å…¥ #{attempt+1}, è¾¨è­˜çµæœ: {code}")

            # 3. è¼¸å…¥è³‡æ–™
            driver.find_element(By.CSS_SELECTOR, 'input[name="SID"]').clear()
            driver.find_element(By.CSS_SELECTOR, 'input[name="SID"]').send_keys(account)
            
            driver.find_element(By.CSS_SELECTOR, 'input[name="PASSWD"]').clear()
            driver.find_element(By.CSS_SELECTOR, 'input[name="PASSWD"]').send_keys(password)
            
            driver.find_element(By.NAME, "ValidCode").clear()
            driver.find_element(By.NAME, "ValidCode").send_keys(code)

            # 4. é€å‡º (æ ¹æ“šä½ çš„ code èª¿æ•´ selector)
            try:
                submit_btn = driver.find_element(By.CSS_SELECTOR, 'input.login_btn_01')
            except:
                # å‚™ç”¨æ–¹æ¡ˆ
                submit_btn = driver.find_element(By.CSS_SELECTOR, 'input[type="submit"]')
            submit_btn.click()

            # 5. è™•ç† Alert (é©—è­‰ç¢¼éŒ¯èª¤æˆ–ç™»å…¥å¤±æ•—)
            try:
                WebDriverWait(driver, 2).until(EC.alert_is_present())
                alert = driver.switch_to.alert
                alert_text = alert.text
                print(f"Alert: {alert_text}")
                
                alert.accept() # é—œé–‰è¦–çª—
                
                if "é©—è­‰ç¢¼éŒ¯èª¤" in alert_text or "Verified Code Error" in alert_text:
                    driver.get(url) # é‡æ–°æ•´ç†æ›æ–°é©—è­‰ç¢¼
                    continue
                else:
                    return False, f"ç™»å…¥å¤±æ•—: {alert_text}"
            except:
                # æ²’æœ‰ Alertï¼Œæª¢æŸ¥æ˜¯å¦è·³è½‰
                if "sco_query.asp" in driver.current_url or "Main" in driver.title:
                    return True, "ç™»å…¥æˆåŠŸ"
                else:
                    # æœ‰æ™‚å€™æ²’è·³è½‰ä¹Ÿæ²’alertï¼Œå¯èƒ½æ˜¯æˆåŠŸ
                     return True, "ç™»å…¥æˆåŠŸ(é åˆ¤)"

        except Exception as e:
            print(f"ç™»å…¥éç¨‹ç•°å¸¸: {e}")
            driver.refresh()
            
    return False, "è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œé©—è­‰ç¢¼è¾¨è­˜å¤±æ•—"

# =====================
# çˆ¬èŸ²é‚è¼¯ (Requests)
# =====================
def get_requests_session_with_cookies(driver):
    session = requests.Session()
    selenium_cookies = driver.get_cookies()
    for cookie in selenium_cookies:
        session.cookies.set(cookie['name'], cookie['value'])
    return session

def scrape_score(driver):
    """
    å°æ‡‰åŸ scrape_all_courses (é–‹æ”¾æˆç¸¾/ç•¶å‰å­¸æœŸæˆç¸¾)
    å›å‚³: List of Dict
    """
    print("ğŸš€ åŸ·è¡Œ: scrape_score")
    list_url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=813&KIND=1&LANGS=cht"
    query_url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?ACTION=814&KIND=1&LANGS=cht"
    
    session = get_requests_session_with_cookies(driver)
    headers = {
        "User-Agent": "Mozilla/5.0", 
        "Referer": list_url,
        "Origin": "https://selcrs.nsysu.edu.tw"
    }

    # 1. å–å¾—èª²ç¨‹æ¸…å–®
    course_info_map = {}
    try:
        res_list = session.get(list_url, headers=headers, timeout=10)
        res_list.encoding = 'utf-8' # æˆ– big5
        soup_list = BeautifulSoup(res_list.text, 'html.parser')
        rows = soup_list.find_all('tr')[1:]
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                course_no = cols[1].get_text(strip=True)
                course_name = cols[2].get_text(strip=True)
                if course_no and not re.search(r'[\u4e00-\u9fff]', course_no):
                    course_info_map[course_no] = course_name
    except Exception as e:
        return {"error": f"å–å¾—èª²ç¨‹æ¸…å–®å¤±æ•—: {str(e)}"}

    # 2. è©³ç´°æˆç¸¾
    results = []
    for course_no, course_name in course_info_map.items():
        payload = {"CRSNO": course_no, "SCO_TYP_COD": "--"}
        try:
            resp = session.post(query_url, headers=headers, data=payload, timeout=10)
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.text, 'html.parser')
            rows = soup.find_all('tr')
            for row in rows:
                cols = [c.get_text(strip=True) for c in row.find_all('td')]
                if len(cols) >= 6 and cols[0].isdigit():
                    results.append({
                        "course_name": course_name,
                        "year": cols[0],
                        "semester": cols[1],
                        "item": cols[2],
                        "percentage": cols[3],
                        "raw_score": cols[4],
                        "grade": cols[5],
                        "note": cols[6] if len(cols)>6 else ""
                    })
        except:
            pass
            
    return results

def scrape_grades(driver):
    """
    å°æ‡‰åŸ scrape_historical_data (æ­·å¹´æˆç¸¾)
    å›å‚³: Dict åŒ…å« "grades"(æˆç¸¾å–®) å’Œ "ranks"(æ’å)
    """
    print("ğŸš€ åŸ·è¡Œ: scrape_grades")
    session = get_requests_session_with_cookies(driver)
    url = "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?ACTION=804&KIND=2&LANGS=cht"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=702"
    }

    years = ["113", "112", "111", "110"]
    sems = ["0", "1", "2", "3"]
    
    all_grades = []
    all_ranks = []

    for year in years:
        for sem in sems:
            payload = {"SYEAR": year, "SEM": sem}
            try:
                response = session.post(url, headers=headers, data=payload, timeout=5)
                response.encoding = response.apparent_encoding # è™•ç†ç·¨ç¢¼
                
                if "ç„¡æ­¤å­¸æœŸæˆç¸¾" in response.text:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # A. æˆç¸¾è¡¨
                rows = soup.find_all('tr')
                for row in rows:
                    cols = [td.get_text(strip=True) for td in row.find_all('td')]
                    # åˆ¤æ–·é‚è¼¯
                    if len(cols) >= 6 and year in cols[0]:
                        all_grades.append({
                            "year": year,
                            "sem": sem,
                            "year_raw": cols[0],
                            "sem_raw": cols[1],
                            "course_id": cols[2],
                            "course_name": cols[3],
                            "credits": cols[4],
                            "score": cols[5]
                        })

                # B. æ’åçµ±è¨ˆ (è§£ææ–‡å­—)
                if "ä¿®ç¿’å­¸åˆ†" in response.text:
                    for table in soup.find_all('table'):
                        txt = table.get_text()
                        if "ä¿®ç¿’å­¸åˆ†" in txt:
                            # Regex æå–
                            clean_text = txt.replace("\n", " ").replace("&nbsp;", "")
                            patterns = {
                                "taken_credits": r"ä¿®ç¿’å­¸åˆ†ï¼š(\d+)",
                                "earned_credits": r"å¯¦å¾—å­¸åˆ†ï¼š(\d+)",
                                "avg_score": r"å¹³å‡åˆ†æ•¸ï¼š([\d\.]+)",
                                "class_rank": r"æœ¬å­¸æœŸåæ¬¡ï¼š(\d+)",
                                "class_size": r"å…¨ç­äººæ•¸ï¼š(\d+)"
                            }
                            rank_data = {"year": year, "sem": sem}
                            for key, p in patterns.items():
                                match = re.search(p, clean_text)
                                rank_data[key] = match.group(1) if match else "N/A"
                            all_ranks.append(rank_data)
                            break
            except Exception as e:
                print(f"æ­·å¹´æˆç¸¾éŒ¯èª¤ ({year}-{sem}): {e}")

    return {"grades": all_grades, "ranks": all_ranks}

# =====================
# API è·¯ç”±
# =====================
@app.post("/crawl")
async def start_crawl(req: CrawlRequest):
    if req.task == "test":
        return {"status": "success", "message": "API is working"}
    
    if not req.account or not req.password:
        raise HTTPException(status_code=422, detail="å¸³è™Ÿèˆ‡å¯†ç¢¼ç‚ºå¿…å¡«æ¬„ä½")

    # è¼‰å…¥æ¨¡å‹ (å»ºè­°åœ¨ startup event è¼‰å…¥ä¸€æ¬¡å…¨åŸŸä½¿ç”¨ï¼Œé€™è£¡ç‚ºæ±‚ç°¡ä¾¿æ¯æ¬¡è¼‰å…¥)
    # è‹¥è«‹æ±‚é‡å¤§ï¼Œè«‹å°‡ load_tflite_model ç§»è‡³ app startup
    interpreter, input_details, output_details = load_tflite_model()
    
    if interpreter is None:
        raise HTTPException(status_code=500, detail="ä¼ºæœå™¨ç«¯ç¼ºå°‘ TFLite æ¨¡å‹æª”æ¡ˆ")

    driver = None
    data = {}
    
    try:
        driver = get_driver()
        
        # åŸ·è¡Œç™»å…¥
        success, msg = login_process(driver, interpreter, input_details, output_details, req.account, req.password)
        if not success:
            return {"status": "failed", "message": msg}
        
        # åŸ·è¡Œä»»å‹™
        # 1. é–‹æ”¾æˆç¸¾ / èª²ç¨‹ç´°é …æˆç¸¾
        if req.task in ["score", "both"]:
            # å…ˆåˆ‡æ›åˆ°é–‹æ”¾æˆç¸¾é é¢ä»¥æ›´æ–° Session ç‹€æ…‹
            driver.get("https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=700&KIND=1&LANGS=cht")
            data["score_task"] = scrape_score(driver)
        
        # 2. æ­·å¹´æˆç¸¾ / å­¸æœŸç¸½æˆç¸¾
        if req.task in ["grades", "both"]:
            # å…ˆåˆ‡æ›åˆ°æ­·å¹´æˆç¸¾é é¢ä»¥æ›´æ–° Session ç‹€æ…‹
            driver.get("https://selcrs.nsysu.edu.tw/scoreqry/sco_query.asp?action=700&KIND=2&LANGS=cht")
            data["grades_task"] = scrape_grades(driver)

        return {"status": "success", "results": data}
    
    except Exception as e:
        return {"status": "error", "message": f"åŸ·è¡Œä¸­æ–·: {str(e)}"}
    
    finally:
        if driver:
            driver.quit()

if __name__ == "__main__":
    import uvicorn
    # ç¢ºä¿ model.tflite åœ¨åŒä¸€ç›®éŒ„ä¸‹
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))